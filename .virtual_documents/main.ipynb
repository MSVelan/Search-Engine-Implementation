import pandas as pd


import requests 

docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'
docs_response = requests.get(docs_url)
documents_raw = docs_response.json()

documents_raw


documents = []

for course in documents_raw:
    course_name = course['course']

    for doc in course['documents']:
        doc['course'] = course_name
        documents.append(doc)


df = pd.DataFrame(documents, columns=['course', 'section', 'question', 'text'])
df.head()


df[df["course"] == "data-engineering-zoomcamp"]








docs_example = [
    "Course starts on 15th Jan 2024",
    "Prerequisites listed on GitHub",
    "Submit homeworks after start date",
    "Registration not required for participation",
    "Setup Google Cloud and Python before course"
]


from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words='english')
X = cv.fit_transform(docs_example)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.toarray(), columns=names).T
df_docs


# TfidfVectorizer is used in giving more importance to less frequently used words
from sklearn.feature_extraction.text import TfidfVectorizer

cv = TfidfVectorizer(stop_words='english')
X = cv.fit_transform(docs_example)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.toarray(), columns=names).T
df_docs.round(2)





from sklearn.feature_extraction.text import TfidfVectorizer

cv = TfidfVectorizer(stop_words='english', min_df=5)
X = cv.fit_transform(df.text)

names = cv.get_feature_names_out()

df_docs = pd.DataFrame(X.toarray(), columns=names)
df_docs.round(2)


query = "Do I need to know python to sign up for the January course?"

q = cv.transform([query])
q.toarray()


query_dict = dict(zip(names, q.toarray()[0]))
query_dict


doc_dict = dict(zip(names, X.toarray()[1]))
doc_dict


df_qd = pd.DataFrame([query_dict, doc_dict], index=['query', 'doc']).T
(df_qd['query'] * df_qd['doc']).sum()


X.shape, q.shape


X.dot(q.T).toarray()


from sklearn.metrics.pairwise import cosine_similarity


score = cosine_similarity(X, q).flatten()


import numpy as np


top_five_search_indices = np.argsort(score)[-5:]


df.iloc[top_five_search_indices]


print(df.iloc[top_five_search_indices[4]]["text"])





fields = ['section', 'question', 'text']


matrices = {}
vectorizers = {}
for f in fields:
    cv = TfidfVectorizer(stop_words='english', min_df=5)
    X = cv.fit_transform(df[f])
    matrices[f] = X
    vectorizers[f] = cv



matrices


n = len(df)
score = np.zeros(n)
query = "I just discovered the course, is it too late to join?"
for f in fields:
    q = vectorizers[f].transform([query])
    X = matrices[f]
    f_score = cosine_similarity(X, q).flatten()
    score = score + f_score
    


top_ids = np.argsort(score)[-5:]


top_ids


df.iloc[top_ids]





filters = {
    'course': "data-engineering-zoomcamp"
}


for field, value in filters.items():
    mask = (df[field] == value).astype(int).values
    score = score * mask

score


top_five_search_indices = np.argsort(score)[-5:]


top_five_search_indices


df.iloc[top_five_search_indices]





boosts = {
    "question": 3
}
n = len(df)
score = np.zeros(n)
query = "I just discovered the course, is it too late to join?"
for f in fields:
    q = vectorizers[f].transform([query])
    X = matrices[f]
    f_score = cosine_similarity(X, q).flatten()
    boost = boosts.get(f, 1.0)
    score = score + f_score * boost
    


# Here filtering is not used
score


idx = np.argsort(score)[-5:]
df.iloc[idx]





class TextSearch:

    def __init__(self, text_fields):
        self.text_fields = text_fields
        self.matrices = {}
        self.vectorizers = {}

    def fit(self, records, vectorizer_params={}):
        self.df = pd.DataFrame(records)

        for f in self.text_fields:
            cv = TfidfVectorizer(**vectorizer_params)
            X = cv.fit_transform(self.df[f])
            self.matrices[f] = X
            self.vectorizers[f] = cv

    def search(self, query, n_results=10, boost={}, filters={}):
        score = np.zeros(len(self.df))

        for f in self.text_fields:
            b = boost.get(f, 1.0)
            q = self.vectorizers[f].transform([query])
            s = cosine_similarity(self.matrices[f], q).flatten()
            score = score + b * s

        for field, value in filters.items():
            mask = (self.df[field] == value).values
            score = score * mask

        idx = np.argsort(-score)[:n_results]
        results = self.df.iloc[idx]
        return results.to_dict(orient='records')


index = TextSearch(
    text_fields=['section', 'question', 'text']
)
index.fit(documents)

index.search(
    query='I just singned up. Is it too late to join the course?',
    n_results=5,
    boost={'question': 3.0},
    filters={'course': 'data-engineering-zoomcamp'}
)


# The above text search is also called as MinSearch, this code is just an implementation of how textsearch works and it need not be used in production



